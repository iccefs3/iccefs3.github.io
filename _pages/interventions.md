# SUMMATIVE EVALUATION

The formative evaluation helped us roadmap implementations, craft learning materials, and plan an initial summative evaluation.

## Study Design
To produce evidence supporting answers to RQ1-2, we organized a 10-step between-subjects study via Mechanical Turk [59].  The steps involved:
1. creation of a username in the learning system;
1. a background survey;
1. review of a 6-minute video tutorial on the UI and CT concept sequences;
1. a pretest;
1. pretest feedback;
1. familiarization and training varied by condition;
1. training feedback;
1. a posttest;
1. posttest feedback;
1. study feedback.

These required responses to the validated CS cognitive load component survey (CS CLCS) [60], to a programming attitude Likert scale survey derived from categorized text-based responses by adults learners in [61], and to the intrinsic motivation Task Evaluation Questionnaire (TEQ) [62].  In step 6, participants followed written instructions to guide their solving of four puzzles; instructions for the first puzzle included a graphical representation of the correct solution and an explanation of the behavior of each block used for the purpose of familiarization.  Each puzzle auto-submitted upon correct completion or after 500 seconds if the participant had not previously submitted an incorrect solution.  We advised participants to complete steps 4, 6, and 8 without interruption and required completion of all steps within two hours.
We randomly assigned participants to one of three conditions operating as the independent variable: 1) PPP training; 2) PPP with distractors training; 3) training by solving puzzles with access to all blocks and without move correctness or score feedback. The dependent variables included time spent and performance on the pretests and posttests, time spent and block moves made in puzzles, and the cognitive load, programming attitude, and TEQ results.

Materials
We tested and refined our materials in collaboration with a high school teacher, 16 of her freshman physics students who had little prior exposure to CT, and eight undergraduates who had diverse declared majors inclusive of CS.  Tests included trials of the surveys and puzzles, as well as think-alouds during which a researcher would observe the participant interacting with puzzles while verbalizing her thoughts.  Results led to refinements such as puzzle theme modification, normalization of pretest/posttest difficulty, and simplification of language used in survey questions.

Participants
In alignment with Wing's mobilizing declaration that CT is a "fundamental skill for everyone, not just for computer scientists" [1], and with the interventionist spirit of design-based research [63], we sought a learner population inclusive of those who might not otherwise encounter an opportunity to engage purposefully with CT but regardless might influence its trajectory in the lives of children. By presenting our study as a Human Intelligence Task (HIT) on Amazon Mechanical Turk, we recruited from a general population of over 100K individuals [64] 75 adults with varying educational experience (24% graduated high school, 60% earned an undergraduate degree, 16% earned a graduate degree) and the variety of self-reported programming experience detailed in Figure 3.  46 men and 29 women comprise the sample population sourced from eight countries including the U.S. (60%), India (20%), and Brazil (11%). While satisfied with this online recruitment for the pilot purpose, less than 12% of submitted HITs provided evidence that all 10 steps of the protocol were sufficiently followed; we have limited our analysis to that set. For our first follow-up study using a similar protocol, we opted instead to recruit participants via Prolific [65], and found over 82% of submissions were sufficiently complete to facilitate analysis.
